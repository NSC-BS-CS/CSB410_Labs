{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cExwSJ5jXTnD",
        "outputId": "bd525133-1159-4b1e-d29e-57d2f62ae595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No Dropout Output: [[0.   1.35 0.  ]]\n",
            "Pre-Dropout Activation: [[0.   1.35 0.  ]]\n",
            "Dropout Mask: [[1. 1. 0.]]\n",
            "With Dropout Output: [[0.  2.7 0. ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Input vector\n",
        "X = np.array([[1.0, 0.5, -1.5]])\n",
        "\n",
        "# Weight matrix (3 neurons in the layer)\n",
        "W = np.array([[0.2, -0.4, 0.1],\n",
        "              [0.5, 0.3, -0.6],\n",
        "              [-0.3, 0.8, 0.7]])\n",
        "\n",
        "# Bias for each neuron\n",
        "b = np.array([0.1, -0.2, 0.3])\n",
        "\n",
        "# ReLU activation\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Forward pass without dropout\n",
        "def forward_no_dropout(X, W, b):\n",
        "    z = np.dot(X, W.T) + b\n",
        "    return relu(z)\n",
        "\n",
        "# Forward pass with dropout (inverted)\n",
        "def forward_with_dropout(X, W, b, dropout_rate=0.5):\n",
        "    z = np.dot(X, W.T) + b\n",
        "    a = relu(z)\n",
        "    mask = (np.random.rand(*a.shape) > dropout_rate).astype(float)\n",
        "    a_dropout = a * mask / (1 - dropout_rate)\n",
        "    return a, mask, a_dropout\n",
        "\n",
        "# Run both\n",
        "no_dropout_output = forward_no_dropout(X, W, b)\n",
        "pre_dropout_output, dropout_mask, with_dropout_output = forward_with_dropout(X, W, b)\n",
        "\n",
        "print(\"No Dropout Output:\", no_dropout_output)\n",
        "print(\"Pre-Dropout Activation:\", pre_dropout_output)\n",
        "print(\"Dropout Mask:\", dropout_mask)\n",
        "print(\"With Dropout Output:\", with_dropout_output)"
      ]
    }
  ]
}