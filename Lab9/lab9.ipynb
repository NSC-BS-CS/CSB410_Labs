{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5554cb5d",
   "metadata": {},
   "source": [
    "# Lab 9\n",
    "\n",
    "— Generative Adversarial Networks (GANs)\n",
    "\n",
    "Build and train a **vanilla GAN** using the code stored in the following file. Be aware, the code you write here and its instructions is not going to be the same expected in the file below.\n",
    "\n",
    "- `10-generative_adversarial_network.ipynb`\n",
    "  and concepts from the slides: `9 - GANs.pptx`.\n",
    "\n",
    "Each task below includes **explicit steps**, **where to find matching content** in the script, and the **expected output**.\n",
    "\n",
    "\\*\\* A note about architectures\n",
    "If the neural network architecture isn't clearly specified, you are given freedom to experiment. Do not take my output results as the \"solution\" to try to achieve but strive for what I have or even better! But even if you are 5% off and lower, that doesn't mean you have done it wrong, you have done it differently. As long as the specified parameters required in the instructions are present, you will get full points.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/username/repo/blob/main/path-to-file)  \n",
    "**Students:** Replace `username`, `repo`, and `path-to-file` with your own GitHub username, repository name, and the path to this file.  \n",
    "After opening in Colab, go to **File → Save a copy to GitHub** (your repo) before editing.\n",
    "\n",
    "#### NOTE: You will need to use a GPU on Google Colab for this to run in a reasonable amount of time without memory running out. I suggest using A100 GPU. Edit -> Notebook Settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17909278",
   "metadata": {},
   "source": [
    "## H.1 Data Preparation — MNIST 28×28 Grayscale\n",
    "\n",
    "**Reference script:** `10-generative_adversarial_network.ipynb`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c6e4e",
   "metadata": {},
   "source": [
    "**Where to find in the script:**\n",
    "\n",
    "- Imports and dataset load for MNIST not in reference file.\n",
    "- Reshaping done in In [51]. May require additional steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (H.1): Load MNIST, scale to [0,1], and reshape to (N,28,28,1).\n",
    "# Steps:\n",
    "# 1) Use keras.datasets.mnist.load_data()\n",
    "# 2) Convert to float32 and divide by 255.0\n",
    "# 3) Expand dimensions to add channel axis\n",
    "# Output variables: x_train (float32, shape=(60000,28,28,1))\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12a03d",
   "metadata": {},
   "source": [
    "**Expected output (H.1):**\n",
    "\n",
    "- `x_train.shape == (60000, 28, 28, 1)`\n",
    "- `x_train.min() >= 0.0` and `x_train.max() <= 1.0`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454848c1",
   "metadata": {},
   "source": [
    "## H.2 Define the Discriminator\n",
    "\n",
    "**Reference script:** `10-generative_adversarial_network.ipynb`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf8e9b4",
   "metadata": {},
   "source": [
    "**Where to find in the script:**\n",
    "\n",
    "- Code in the file uses a special model building class\n",
    "- Refer to previous scripts for building a Sequential\n",
    "- as a hint here is a start: model = keras.Sequential([\n",
    "- Follow instructions below\n",
    "- Optimizer setup for discriminator (often Adam with β1≈0.5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab0cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (H.2): Build the discriminator.\n",
    "# Architecture (match script style):\n",
    "#   Input: (28,28,1)\n",
    "#   Conv2D(64, kernel_size=5, strides=2, padding='same') → LeakyReLU(0.2) → Dropout(0.3)\n",
    "#   Conv2D(128, kernel_size=5, strides=2, padding='same') → LeakyReLU(0.2) → Dropout(0.3)\n",
    "#   Flatten → Dense(1, activation='sigmoid')\n",
    "# Compile with loss='binary_crossentropy', optimizer=Adam(lr ~ 2e-4, beta_1 ~ 0.5), metrics=['accuracy']\n",
    "# Name your model variable: discriminator\n",
    "# Print the model summary: discriminator.summary()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b06db9",
   "metadata": {},
   "source": [
    "**Expected output (H.2):**\n",
    "\n",
    "- `discriminator` is a compiled Keras model.\n",
    "- Calling `discriminator(tf.zeros([1,28,28,1]))` returns a (1,1) tensor in [0,1].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2639125f",
   "metadata": {},
   "source": [
    "## H.3 Define the Generator\n",
    "\n",
    "**Reference script:** `10-generative_adversarial_network.ipynb`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f2967b",
   "metadata": {},
   "source": [
    "**Where to find in the script:**\n",
    "\n",
    "- Again code in that script create from a special model building function\n",
    "- Create yours as we have been doing using Sequential\n",
    "- Hint: model = keras.Sequential([\n",
    "- Latent vector size (e.g., `latent_dim = 100`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f499ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (H.3): Build the generator that maps z∼N(0,1) of shape (latent_dim,) to (28,28,1).\n",
    "# Suggested pattern (match script):\n",
    "#   Dense(7*7*128) → LeakyReLU → Reshape(7,7,128)\n",
    "#   Conv2DTranspose(64, kernel_size=5, strides=2, padding='same') → LeakyReLU\n",
    "#   Conv2DTranspose(1,  kernel_size=5, strides=2, padding='same', activation='sigmoid')\n",
    "# Name your model variable: generator\n",
    "# Print the model summary: generator.summary()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc609b4",
   "metadata": {},
   "source": [
    "**Expected output (H.3):**\n",
    "\n",
    "- `generator` is a Keras model that, given a (batch, latent_dim) noise input, outputs images of shape (batch,28,28,1) in [0,1].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5dedf8",
   "metadata": {},
   "source": [
    "## H.4 Adversarial (GAN) Model\n",
    "\n",
    "**Reference script:** `10-generative_adversarial_network.ipynb`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc8633",
   "metadata": {},
   "source": [
    "**Where to find in the script:**\n",
    "\n",
    "- Follow along with instructions below. Adversarial building is different in the 10-...script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b255e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (H.4): Build the GAN by stacking generator → discriminator.\n",
    "# Steps:\n",
    "# 1) Set discriminator.trainable = False (for the GAN compile step)\n",
    "# 2) Create an Input for z (latent_dim,)\n",
    "# Consider keras.Input for this\n",
    "# 3) Pass through generator then discriminator\n",
    "# Hint will look something like this\n",
    "# img = generator(z_in)\n",
    "# score = discriminator(img)\n",
    "# 4) Compile GAN with loss='binary_crossentropy' and Adam(lr ~ 2e-4, beta_1 ~ 0.5)\n",
    "# Name your model variable: gan\n",
    "# Print the model summary: gan.summary()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd1e89",
   "metadata": {},
   "source": [
    "**Expected output (H.4):**\n",
    "\n",
    "- `gan` is a compiled model that takes noise and returns a real/fake score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6cb8a7",
   "metadata": {},
   "source": [
    "## H.5 Training Loop (Memory-aware)\n",
    "\n",
    "**Reference script:** `10-generative_adversarial_network.ipynb`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a98a2fb",
   "metadata": {},
   "source": [
    "**Where to find in the script:**\n",
    "\n",
    "- The custom loop alternating discriminator and generator updates (often using real labels=1 and fake labels=0).\n",
    "- Epoch logging and periodic sampling of generated images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c39c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the full GAN training loop.\n",
    "#\n",
    "# Use the reference training function provided in the lab description as a GUIDE.\n",
    "# Your loop here should follow the same conceptual steps even though the exact\n",
    "# shapes, labels, and method calls may differ depending on your generator,\n",
    "# discriminator, and GAN models.\n",
    "#\n",
    "# -------------------------------------------------------------------------\n",
    "# The reference function illustrates the following key ideas:\n",
    "# \n",
    "# 1. SAMPLE REAL IMAGES:\n",
    "#    - Pull a batch of real images from your training data.\n",
    "#    - Reshape them if needed to (batch, height, width, channels).\n",
    "#\n",
    "# 2. GENERATE FAKE IMAGES:\n",
    "#    - Sample random noise from a uniform or normal distribution.\n",
    "#    - Pass this noise through the generator to create fake images.\n",
    "#\n",
    "# 3. PREPARE DISCRIMINATOR INPUTS:\n",
    "#    - Combine real and fake images into a single batch for training.\n",
    "#    - Create labels:\n",
    "#          real images → label 1\n",
    "#          fake images → label 0\n",
    "#\n",
    "# 4. TRAIN THE DISCRIMINATOR:\n",
    "#    - Train on real + fake inputs in one step OR train separately on\n",
    "#      real images and fake images (as shown in the more modern GAN loop).\n",
    "#    - Track the discriminator's loss and accuracy.\n",
    "#\n",
    "# 5. TRAIN THE GENERATOR THROUGH THE GAN MODEL:\n",
    "#    - Sample a new batch of random noise.\n",
    "#    - Assign labels of 1 (meaning “real”) to that noise batch.\n",
    "#      Why? Because the generator wants to fool the discriminator.\n",
    "#    - Freeze the discriminator’s weights so that GAN training updates ONLY\n",
    "#      the generator.\n",
    "#\n",
    "# 6. LOG METRICS:\n",
    "#    - Track generator losses and discriminator losses every epoch.\n",
    "#    - Print summary statistics periodically (e.g., every 5 epochs).\n",
    "#\n",
    "# 7. VISUALIZE GENERATED IMAGES:\n",
    "#    - Use a function like sample_grid() to visualize progress.\n",
    "#    - Always generate from a FIXED noise vector so that improvements\n",
    "#      over epochs are easy to see.\n",
    "#\n",
    "# -------------------------------------------------------------------------\n",
    "# HOW TO MAP THE REFERENCE FUNCTION TO YOUR TRAINING LOOP HERE:\n",
    "#\n",
    "# In the reference code:\n",
    "#     real_imgs = <sampled real batch>\n",
    "#     fake_imgs = generator.predict(...)\n",
    "#     d_metrics = discriminator.train_on_batch(...)\n",
    "#     a_metrics = adversarial_model.train_on_batch(...)\n",
    "#\n",
    "# In your loop for this lab, use:\n",
    "#     x_real  = sampled real images\n",
    "#     x_fake  = generator(z)\n",
    "#     discriminator.train_on_batch(...)\n",
    "#     gan.train_on_batch(...)\n",
    "#\n",
    "# The IDEA is the same:\n",
    "#   - Discriminator gets real+fake with 1s and 0s.\n",
    "#   - Generator gets noise but uses label=1 to push it toward realism.\n",
    "#\n",
    "# -------------------------------------------------------------------------\n",
    "# STRUCTURE TO FOLLOW:\n",
    "#\n",
    "# for epoch in range(1, EPOCHS+1):\n",
    "#     Initialize lists for tracking losses\n",
    "#\n",
    "#     for step in range(steps_per_epoch):\n",
    "#         -----------------------------\n",
    "#         (A) Train Discriminator\n",
    "#         -----------------------------\n",
    "#         - Sample real images\n",
    "#         - Generate fake images\n",
    "#         - Train discriminator on both real and fake batches\n",
    "#           (you may call train_on_batch twice)\n",
    "#\n",
    "#         -----------------------------\n",
    "#         (B) Train Generator (via GAN)\n",
    "#         -----------------------------\n",
    "#         - Sample new random noise\n",
    "#         - Train GAN with label=1\n",
    "#           (this updates ONLY the generator)\n",
    "#\n",
    "#     Print epoch metrics\n",
    "#\n",
    "#     Every N epochs (e.g., 5):\n",
    "#         sample_grid(generator, fixed_noise)\n",
    "#\n",
    "# -------------------------------------------------------------------------\n",
    "# IMPORTANT:\n",
    "# - The discriminator alternates between training on real and fake images.\n",
    "# - The generator learns by trying to fool the discriminator.\n",
    "# - Freezing/unfreezing discriminator.trainable at the right times matters.\n",
    "# - Even though the TRAINING STYLE is similar to the reference function,\n",
    "#   use the architecture and dataset defined in THIS lab.\n",
    "#\n",
    "# -------------------------------------------------------------------------\n",
    "# Implement your full training loop below following this structure.\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961fdb6f",
   "metadata": {},
   "source": [
    "**Expected output (H.5):**\n",
    "\n",
    "- Per-epoch logs: D loss/acc and GAN (generator) loss.\n",
    "- A few image grids showing samples improving over time.\n",
    "- Final print of losses across epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3759fcd",
   "metadata": {},
   "source": [
    "## Discussion Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bafd4f",
   "metadata": {},
   "source": [
    "1. Why can GAN training be unstable? Name two symptoms and one stabilization trick.\n",
    "2. What happens if the discriminator is much stronger than the generator? How would you adjust training?\n",
    "3. If generated digits are blurry, which change to the generator would you try first, and why?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
